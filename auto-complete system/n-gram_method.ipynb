{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topics = [\n",
    "    \"Artificial Intelligence\",\n",
    "    \"Climate Change\",\n",
    "    \"Quantum Computing\",\n",
    "    \"World War II\",\n",
    "    \"Ancient Egypt\",\n",
    "    \"Space Exploration\",\n",
    "    \"Global Health\",\n",
    "    \"Economics\",\n",
    "    \"Philosophy of Science\",\n",
    "    \"Modern Art\",\n",
    "    \"Genetics\",\n",
    "    \"Renewable Energy\",\n",
    "    \"Cybersecurity\",\n",
    "    \"Cryptocurrency\",\n",
    "    \"Social Media\",\n",
    "    \"Cultural Anthropology\",\n",
    "    \"Astrophysics\",\n",
    "    \"Human Rights\",\n",
    "    \"Machine Learning\",\n",
    "    \"History of Technology\",\n",
    "    \"Biotechnology\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_corpus(corpus):\n",
    "    sentences=corpus.split('.')\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    return sentences\n",
    "def tokenize_sentences(sentences):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokenized = sentence.split(' ')\n",
    "        final_tokenized=[]\n",
    "        for word in tokenized:\n",
    "            if len(word)>0:\n",
    "                if (word[-1]>='a' and word[-1]<='z') or word[-1]=='>':\n",
    "                    final_tokenized.append(word)\n",
    "                else:\n",
    "                    final_tokenized.append(word[:-1])\n",
    "        tokenized_sentences.append(final_tokenized)\n",
    "        \n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "def get_vocabulary_bigram(data,min_freq=2):\n",
    "    word_counts={}  #keys:word and value:count\n",
    "    vocab=[]\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            if word in word_counts.keys():\n",
    "                word_counts[word]+=1\n",
    "            else:\n",
    "                word_counts[word]=1\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            if word_counts[word]>min_freq and word not in vocab:\n",
    "                vocab.append(word)\n",
    "    return vocab,word_counts\n",
    "\n",
    "\n",
    "def get_vocabulary_trigram(data,min_freq=2):\n",
    "    word_counts={}  #keys:word and value:count\n",
    "    vocab=[]\n",
    "    for sentence in data:\n",
    "        for i in range(len(sentence)-1):\n",
    "            if (sentence[i],sentence[i+1]) in word_counts.keys():\n",
    "                word_counts[(sentence[i],sentence[i+1])]+=1\n",
    "            else:\n",
    "                word_counts[(sentence[i],sentence[i+1])]=1\n",
    "            vocab.append(sentence[i])\n",
    "            if i==len(sentence)-2:\n",
    "                vocab.append(sentence[i+1])            \n",
    "    return vocab,word_counts\n",
    "def add_starts_ends(data,n=2):\n",
    "    final_data=[]\n",
    "    for sentence in data:\n",
    "        final_data.append(['<s>'] * n + sentence + ['<e>'])\n",
    "    return final_data\n",
    "# def replace_with_unk(data,vocab):\n",
    "#     final_data=[]\n",
    "#     for sentence in data:\n",
    "#         final_sentence=[]\n",
    "#         for word in sentence:\n",
    "#             if word in vocab:\n",
    "#                 final_sentence.append(word)\n",
    "#             else:\n",
    "#                 final_sentence.append('<unk>')\n",
    "#         final_data.append(final_sentence)\n",
    "#     return final_data\n",
    "def calculate_probability_birgrams(data,vocab_size,word_counts,k=3):\n",
    "    count_bigrams={}\n",
    "    for sentence in data:\n",
    "        for i in range(len(sentence)-1):\n",
    "            if (sentence[i],sentence[i+1]) in count_bigrams.keys():\n",
    "                count_bigrams[(sentence[i],sentence[i+1])]+=1\n",
    "            else:\n",
    "                count_bigrams[(sentence[i],sentence[i+1])]=1\n",
    "    probability_bigrams={}\n",
    "    for key,values in count_bigrams.items():\n",
    "        probability_bigrams[key]=(count_bigrams[key]+k)/(word_counts[key[0]]+k*vocab_size)\n",
    "    return probability_bigrams\n",
    "\n",
    "def calculate_probability_trigrams(data,vocab_size,word_counts,k=3):\n",
    "    count_trigrams={}\n",
    "    for sentence in data:\n",
    "        for i in range(len(sentence)-2):\n",
    "            if (sentence[i],sentence[i+1],sentence[i+2]) in count_trigrams.keys():\n",
    "                count_trigrams[(sentence[i],sentence[i+1],sentence[i+2])]+=1\n",
    "            else:\n",
    "                count_trigrams[(sentence[i],sentence[i+1],sentence[i+2])]=1\n",
    "    probability_trigrams={}\n",
    "    for key,values in count_trigrams.items():\n",
    "        probability_trigrams[key]=(count_trigrams[key]+k)/(word_counts[(key[0],key[1])]+k*vocab_size)\n",
    "    return probability_trigrams\n",
    "def suggest_word(sentence,lm):\n",
    "    max_prob=-10000\n",
    "    suggested_word=''\n",
    "    sentence_log_prob=sum([np.log(lm[(sentence[i],sentence[i+1],sentence[i+2])]) for i in range(len(sentence)-2) if (sentence[i],sentence[i+1],sentence[i+2]) in lm.keys()])\n",
    "    for key,value in lm.items():\n",
    "        if key[0]==sentence[len(sentence)-2] and key[1]==sentence[len(sentence)-1]:\n",
    "            if np.log(value)+sentence_log_prob>max_prob:\n",
    "                max_prob=np.log(value)+sentence_log_prob\n",
    "                suggested_word=key[2]\n",
    "    return suggested_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence\n",
      "Climate Change\n",
      "Quantum Computing\n",
      "World War II\n",
      "Ancient Egypt\n",
      "skipped: Space Exploration\n",
      "Global Health\n",
      "Economics\n",
      "Philosophy of Science\n",
      "Modern Art\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahdi/anaconda3/envs/myenv/lib/python3.11/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/mahdi/anaconda3/envs/myenv/lib/python3.11/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: Genetics\n",
      "Renewable Energy\n",
      "Cybersecurity\n",
      "Cryptocurrency\n",
      "Social Media\n",
      "Cultural Anthropology\n",
      "Astrophysics\n",
      "Human Rights\n",
      "skipped: Machine Learning\n",
      "History of Technology\n",
      "Biotechnology\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus=''\n",
    "for topic in topics:\n",
    "    try:\n",
    "        page = wikipedia.page(topic)\n",
    "        corpus+=page.content\n",
    "        print(topic)\n",
    "    except:\n",
    "        print('skipped:',topic)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=divide_corpus(corpus)\n",
    "data=tokenize_sentences(data)\n",
    "data=add_starts_ends(data)\n",
    "vocab,word_counts=get_vocabulary_trigram(data,3)\n",
    "# data=replace_with_unk(data,vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equipped\n"
     ]
    }
   ],
   "source": [
    "lm=calculate_probability_trigrams(data,len(vocab),word_counts)\n",
    "sentence=\"we are\"\n",
    "sentence=divide_corpus(sentence)\n",
    "sentence=tokenize_sentences(sentence)\n",
    "# sentence=add_starts_ends(sentence)\n",
    "print(suggest_word(sentence[0],lm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> philosophy of science\n"
     ]
    }
   ],
   "source": [
    "sentence=\"<s> philosophy\"\n",
    "prep_sentence=divide_corpus(sentence)\n",
    "prep_sentence=tokenize_sentences(prep_sentence)\n",
    "while suggest_word(prep_sentence[0],lm)!=\"<e>\" and suggest_word(prep_sentence[0],lm)!=\"\":\n",
    "    sentence+=\" \"+suggest_word(prep_sentence[0],lm)\n",
    "    prep_sentence=divide_corpus(sentence)\n",
    "    prep_sentence=tokenize_sentences(prep_sentence)\n",
    "print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
