{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mahdi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipedia\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topics = [\n",
    "    \"Artificial Intelligence\",\n",
    "    \"Climate Change\",\n",
    "    \"Quantum Computing\",\n",
    "    \"World War II\",\n",
    "    \"Ancient Egypt\",\n",
    "    \"Space Exploration\",\n",
    "    \"Global Health\",\n",
    "    \"Economics\",\n",
    "    \"Philosophy of Science\",\n",
    "    \"Modern Art\",\n",
    "    \"Genetics\",\n",
    "    \"Renewable Energy\",\n",
    "    \"Cybersecurity\",\n",
    "    \"Cryptocurrency\",\n",
    "    \"Social Media\",\n",
    "    \"Cultural Anthropology\",\n",
    "    \"Astrophysics\",\n",
    "    \"Human Rights\",\n",
    "    \"Machine Learning\",\n",
    "    \"History of Technology\",\n",
    "    \"Biotechnology\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_corpus(corpus):\n",
    "    sentences=corpus.split('.')\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    return sentences\n",
    "def tokenize_sentences(sentences):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokenized = sentence.split(' ')\n",
    "        final_tokenized=[]\n",
    "        for word in tokenized:\n",
    "            if len(word)>0:\n",
    "                if (word[-1]>='a' and word[-1]<='z') or word[-1]=='>':\n",
    "                    final_tokenized.append(word)\n",
    "                else:\n",
    "                    final_tokenized.append(word[:-1])\n",
    "        tokenized_sentences.append(final_tokenized)\n",
    "        \n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "def get_vocabulary(data,min_freq=2):\n",
    "    word_counts={}  #keys:word and value:count\n",
    "    vocab=[]\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            if word in word_counts.keys():\n",
    "                word_counts[word]+=1\n",
    "            else:\n",
    "                word_counts[word]=1\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            if word_counts[word]>min_freq and word not in vocab:\n",
    "                vocab.append(word)\n",
    "    return vocab,word_counts\n",
    "def add_starts_ends(data,n=1):\n",
    "    final_data=[]\n",
    "    for sentence in data:\n",
    "        final_data.append(['<s>'] * n + sentence + ['<e>'])\n",
    "    return final_data\n",
    "# def replace_with_unk(data,vocab):\n",
    "#     final_data=[]\n",
    "#     for sentence in data:\n",
    "#         final_sentence=[]\n",
    "#         for word in sentence:\n",
    "#             if word in vocab:\n",
    "#                 final_sentence.append(word)\n",
    "#             else:\n",
    "#                 final_sentence.append('<unk>')\n",
    "#         final_data.append(final_sentence)\n",
    "#     return final_data\n",
    "def calculate_probability_birgrams(data,vocab_size,word_counts,k=3):\n",
    "    count_bigrams={}\n",
    "    for sentence in data:\n",
    "        for i in range(len(sentence)-1):\n",
    "            if (sentence[i],sentence[i+1]) in count_bigrams.keys():\n",
    "                count_bigrams[(sentence[i],sentence[i+1])]+=1\n",
    "            else:\n",
    "                count_bigrams[(sentence[i],sentence[i+1])]=1\n",
    "    probability_bigrams={}\n",
    "    for key,values in count_bigrams.items():\n",
    "        probability_bigrams[key]=(count_bigrams[key]+k)/(word_counts[key[0]]+k*vocab_size)\n",
    "    return probability_bigrams\n",
    "def suggest_word(sentence,lm):\n",
    "    max_prob=-10000\n",
    "    suggested_word=''\n",
    "    sentence_log_prob=sum([np.log(lm[(sentence[i],sentence[i+1])]) for i in range(len(sentence)-1) if (sentence[i],sentence[i+1]) in lm.keys()])\n",
    "    for key,value in lm.items():\n",
    "        if key[0]==sentence[len(sentence)-1]:\n",
    "            if np.log(value)+sentence_log_prob>max_prob:\n",
    "                max_prob=np.log(value)+sentence_log_prob\n",
    "                suggested_word=key[1]\n",
    "    return suggested_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus=''\n",
    "for topic in topics:\n",
    "    try:\n",
    "        page = wikipedia.page(topic)\n",
    "        corpus+=page.content\n",
    "        print(topic)\n",
    "    except:\n",
    "        print('skipped:',topic)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=divide_corpus(corpus)\n",
    "data=tokenize_sentences(data)\n",
    "data=add_starts_ends(data)\n",
    "vocab,word_counts=get_vocabulary(data,3)\n",
    "# data=replace_with_unk(data,vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm=calculate_probability_birgrams(data,len(vocab),word_counts)\n",
    "sentence=\"I want to\"\n",
    "sentence=divide_corpus(sentence)\n",
    "sentence=tokenize_sentences(sentence)\n",
    "# sentence=add_starts_ends(sentence)\n",
    "print(suggest_word(sentence[0],lm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"<s>\"\n",
    "prep_sentence=divide_corpus(sentence)\n",
    "prep_sentence=tokenize_sentences(prep_sentence)\n",
    "while suggest_word(prep_sentence[0],lm)!=\"<e>\" or suggest_word(prep_sentence[0],lm)==\"\":\n",
    "    sentence+=\" \"+suggest_word(prep_sentence[0],lm)\n",
    "    prep_sentence=divide_corpus(sentence)\n",
    "    prep_sentence=tokenize_sentences(prep_sentence)\n",
    "    print(sentence)\n",
    "print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
