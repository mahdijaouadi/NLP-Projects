{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll the steps as before, 1) divide the corpus into sentences 2) tokenize the sentences 3) get_vocab to make the one_hot encoding\n",
    "# add starts and ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topics = [\n",
    "    \"Artificial Intelligence\",\n",
    "    \"Climate Change\",\n",
    "    \"Quantum Computing\",\n",
    "    \"World War II\",\n",
    "    \"Ancient Egypt\",\n",
    "    \"Space Exploration\",\n",
    "    \"Global Health\",\n",
    "    \"Economics\",\n",
    "    \"Philosophy of Science\",\n",
    "    \"Modern Art\",\n",
    "    \"Genetics\",\n",
    "    \"Renewable Energy\",\n",
    "    \"Cybersecurity\",\n",
    "    \"Cryptocurrency\",\n",
    "    \"Social Media\",\n",
    "    \"Cultural Anthropology\",\n",
    "    \"Astrophysics\",\n",
    "    \"Human Rights\",\n",
    "    \"Machine Learning\",\n",
    "    \"History of Technology\",\n",
    "    \"Biotechnology\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_corpus(corpus):\n",
    "    sentences=corpus.split('.')\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    return sentences\n",
    "def tokenize_sentences(sentences):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokenized = sentence.split(' ')\n",
    "        final_tokenized=[]\n",
    "        for word in tokenized:\n",
    "            if len(word)>0:\n",
    "                if (word[-1]>='a' and word[-1]<='z') or word[-1]=='>':\n",
    "                    final_tokenized.append(word)\n",
    "                else:\n",
    "                    final_tokenized.append(word[:-1])\n",
    "        tokenized_sentences.append(final_tokenized)\n",
    "        \n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "def get_vocabulary(data,min_freq=1):\n",
    "    word_counts={}  #keys:word and value:count\n",
    "    vocab={}\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            if word in word_counts.keys():\n",
    "                word_counts[word]+=1\n",
    "            else:\n",
    "                word_counts[word]=1\n",
    "    i=0\n",
    "    for key,value in word_counts.items():\n",
    "        if value>=min_freq:\n",
    "            vocab[key]=i\n",
    "            i+=1\n",
    "    return vocab\n",
    "def add_starts_ends(data,n=1):\n",
    "    final_data=[]\n",
    "    for sentence in data:\n",
    "        final_data.append(['<s>'] * n + sentence + ['<e>'])\n",
    "    return final_data\n",
    "def suggest_word(sentence,lm):\n",
    "    max_prob=-10000\n",
    "    suggested_word=''\n",
    "    sentence_log_prob=sum([np.log(lm[(sentence[i],sentence[i+1])]) for i in range(len(sentence)-1) if (sentence[i],sentence[i+1]) in lm.keys()])\n",
    "    for key,value in lm.items():\n",
    "        if key[0]==sentence[len(sentence)-1]:\n",
    "            if np.log(value)+sentence_log_prob>max_prob:\n",
    "                max_prob=np.log(value)+sentence_log_prob\n",
    "                suggested_word=key[1]\n",
    "    return suggested_word\n",
    "\n",
    "def one_hot_encode(word,vocab):\n",
    "    word_encoded=np.zeros(len(vocab))\n",
    "    word_encoded[vocab[word]]=1\n",
    "    return(word_encoded)\n",
    "def get_input(sentence,i,vocab):\n",
    "    sentence_encoded=np.zeros(len(vocab))\n",
    "    for j in range(0,i):\n",
    "        sentence_encoded+=one_hot_encode(sentence[j],vocab)\n",
    "    return(sentence_encoded/(j+1))\n",
    "\n",
    "def prepare_train_data(data,vocab):\n",
    "    y=[]\n",
    "    x=[]\n",
    "    for sentence in data:\n",
    "        for i in range(1,len(sentence)):\n",
    "            y.append(one_hot_encode(sentence[i],vocab))\n",
    "            x.append(get_input(sentence,i,vocab))\n",
    "    return np.array(x),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence\n",
      "Climate Change\n",
      "Quantum Computing\n",
      "World War II\n",
      "Ancient Egypt\n",
      "skipped: Space Exploration\n",
      "Global Health\n",
      "Economics\n",
      "Philosophy of Science\n",
      "Modern Art\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahdi/anaconda3/envs/myenv/lib/python3.11/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/mahdi/anaconda3/envs/myenv/lib/python3.11/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: Genetics\n",
      "Renewable Energy\n",
      "Cybersecurity\n",
      "Cryptocurrency\n",
      "Social Media\n",
      "Cultural Anthropology\n",
      "Astrophysics\n",
      "Human Rights\n",
      "skipped: Machine Learning\n",
      "History of Technology\n",
      "Biotechnology\n"
     ]
    }
   ],
   "source": [
    "corpus=''\n",
    "for topic in topics:\n",
    "    try:\n",
    "        page = wikipedia.page(topic)\n",
    "        corpus+=page.content\n",
    "        print(topic)\n",
    "    except:\n",
    "        print('skipped:',topic)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18385\n"
     ]
    }
   ],
   "source": [
    "data=divide_corpus(corpus)\n",
    "data=tokenize_sentences(data)\n",
    "data=add_starts_ends(data)\n",
    "vocab=get_vocabulary(data,1)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\"\n",
    "class LLM(nn.Module):\n",
    "    def __init__(self,in_features):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, in_features),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "in_features=len(vocab)\n",
    "model=LLM(in_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "accuracy:  0.96875\n",
      "best_accuracy:  0.96875\n",
      "1\n",
      "accuracy:  0.0\n",
      "best_accuracy:  0.96875\n",
      "2\n",
      "accuracy:  0.53125\n",
      "best_accuracy:  0.96875\n",
      "3\n",
      "accuracy:  0.53125\n",
      "best_accuracy:  0.96875\n",
      "4\n",
      "accuracy:  1.0\n",
      "best_accuracy:  1.0\n",
      "5\n",
      "accuracy:  1.0\n",
      "best_accuracy:  1.0\n",
      "6\n",
      "accuracy:  0.96875\n",
      "best_accuracy:  1.0\n",
      "7\n",
      "accuracy:  0.96875\n",
      "best_accuracy:  1.0\n",
      "8\n",
      "accuracy:  0.96875\n",
      "best_accuracy:  1.0\n",
      "9\n",
      "accuracy:  0.96875\n",
      "best_accuracy:  1.0\n",
      "10\n",
      "accuracy:  0.96875\n",
      "best_accuracy:  1.0\n",
      "11\n",
      "accuracy:  0.96875\n",
      "best_accuracy:  1.0\n",
      "12\n",
      "accuracy:  1.0\n",
      "best_accuracy:  1.0\n",
      "13\n",
      "accuracy:  1.0\n",
      "best_accuracy:  1.0\n",
      "14\n",
      "accuracy:  1.0\n",
      "best_accuracy:  1.0\n",
      "15\n",
      "accuracy:  1.0\n",
      "best_accuracy:  1.0\n",
      "16\n",
      "accuracy:  1.0\n",
      "best_accuracy:  1.0\n",
      "17\n",
      "accuracy:  0.96875\n",
      "best_accuracy:  1.0\n",
      "18\n",
      "accuracy:  0.96875\n",
      "best_accuracy:  1.0\n",
      "19\n",
      "accuracy:  0.96875\n",
      "best_accuracy:  1.0\n",
      "20\n",
      "accuracy:  0.96875\n",
      "best_accuracy:  1.0\n",
      "21\n",
      "accuracy:  0.96875\n",
      "best_accuracy:  1.0\n",
      "22\n",
      "accuracy:  0.96875\n",
      "best_accuracy:  1.0\n",
      "23\n",
      "accuracy:  1.0\n",
      "best_accuracy:  1.0\n",
      "24\n",
      "accuracy:  1.0\n",
      "best_accuracy:  1.0\n",
      "25\n",
      "accuracy:  1.0\n",
      "best_accuracy:  1.0\n",
      "26\n",
      "accuracy:  1.0\n",
      "best_accuracy:  1.0\n",
      "27\n",
      "accuracy:  0.53125\n",
      "best_accuracy:  1.0\n",
      "28\n",
      "accuracy:  1.0\n",
      "best_accuracy:  1.0\n",
      "29\n",
      "accuracy:  0.96875\n",
      "best_accuracy:  1.0\n",
      "30\n",
      "accuracy:  0.96875\n",
      "best_accuracy:  1.0\n",
      "31\n",
      "accuracy:  0.96875\n",
      "best_accuracy:  1.0\n",
      "32\n",
      "accuracy:  0.96875\n",
      "best_accuracy:  1.0\n",
      "33\n",
      "accuracy:  0.96875\n",
      "best_accuracy:  1.0\n",
      "34\n",
      "accuracy:  0.96875\n",
      "best_accuracy:  1.0\n",
      "35\n",
      "accuracy:  0.96875\n",
      "best_accuracy:  1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m      7\u001b[0m     j\u001b[38;5;241m=\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(vocab)\u001b[38;5;241m-\u001b[39mBATCH_SIZE)\n\u001b[0;32m----> 8\u001b[0m     x,y\u001b[38;5;241m=\u001b[39m\u001b[43mprepare_train_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     x\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfrom_numpy(x)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     10\u001b[0m     y\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfrom_numpy(y)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "Cell \u001b[0;32mIn[4], line 70\u001b[0m, in \u001b[0;36mprepare_train_data\u001b[0;34m(data, vocab)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(sentence)):\n\u001b[1;32m     69\u001b[0m         y\u001b[38;5;241m.\u001b[39mappend(one_hot_encode(sentence[i],vocab))\n\u001b[0;32m---> 70\u001b[0m         x\u001b[38;5;241m.\u001b[39mappend(\u001b[43mget_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(x),np\u001b[38;5;241m.\u001b[39marray(y)\n",
      "Cell \u001b[0;32mIn[4], line 59\u001b[0m, in \u001b[0;36mget_input\u001b[0;34m(sentence, i, vocab)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input\u001b[39m(sentence,i,vocab):\n\u001b[0;32m---> 59\u001b[0m     sentence_encoded\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(vocab))\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,i):\n\u001b[1;32m     61\u001b[0m         sentence_encoded\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mone_hot_encode(sentence[j],vocab)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS=200\n",
    "BATCH_SIZE=32\n",
    "best_accuracy=0\n",
    "crit=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-2,weight_decay=5e-4)\n",
    "for epoch in range(EPOCHS):\n",
    "    j=random.randint(0,len(vocab)-BATCH_SIZE)\n",
    "    x,y=prepare_train_data(data[0:BATCH_SIZE],vocab)\n",
    "    x=torch.from_numpy(x).float()\n",
    "    y=torch.from_numpy(y).float()\n",
    "    #forward\n",
    "    y_hat=model(x)\n",
    "    loss=crit(y,y_hat)\n",
    "    #backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, predicted = torch.max(y_hat, 1)\n",
    "    y_indices = torch.argmax(y, dim=1)\n",
    "    correct = (predicted == y_indices).sum().item()\n",
    "    total = BATCH_SIZE\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    if accuracy>=best_accuracy:\n",
    "        best_accuracy=accuracy\n",
    "    print(epoch)\n",
    "    print('accuracy: ',accuracy)\n",
    "    print('best_accuracy: ',best_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
