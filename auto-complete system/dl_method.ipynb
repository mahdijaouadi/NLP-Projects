{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll the steps as before, 1) divide the corpus into sentences 2) tokenize the sentences 3) get_vocab to make the one_hot encoding\n",
    "# add starts and ends\n",
    "\n",
    "# First problem\n",
    "#   The model is seeing certain words more than others (The, in, at, on, and)\n",
    "#   There's no order, so the model will find it impossible to find next word, the model know which word he has but he doesn't know the order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topics = [\n",
    "    \"Artificial Intelligence\",\n",
    "    \"Climate Change\",\n",
    "    \"Quantum Computing\",\n",
    "    \"World War II\",\n",
    "    \"Ancient Egypt\",\n",
    "    \"Space Exploration\",\n",
    "    \"Global Health\",\n",
    "    \"Economics\",\n",
    "    \"Philosophy of Science\",\n",
    "    \"Modern Art\",\n",
    "    \"Genetics\",\n",
    "    \"Renewable Energy\",\n",
    "    \"Cybersecurity\",\n",
    "    \"Cryptocurrency\",\n",
    "    \"Social Media\",\n",
    "    \"Cultural Anthropology\",\n",
    "    \"Astrophysics\",\n",
    "    \"Human Rights\",\n",
    "    \"Machine Learning\",\n",
    "    \"History of Technology\",\n",
    "    \"Biotechnology\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_corpus(corpus):\n",
    "    sentences=corpus.split('.')\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    return sentences\n",
    "def tokenize_sentences(sentences):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokenized = sentence.split(' ')\n",
    "        final_tokenized=[]\n",
    "        for word in tokenized:\n",
    "            if len(word)>0:\n",
    "                if (word[-1]>='a' and word[-1]<='z') or word[-1]=='>':\n",
    "                    final_tokenized.append(word)\n",
    "                else:\n",
    "                    final_tokenized.append(word[:-1])\n",
    "        tokenized_sentences.append(final_tokenized)\n",
    "        \n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "def get_vocabulary(data,min_freq=1):\n",
    "    word_counts={}  #keys:word and value:count\n",
    "    vocab={}\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            if word in word_counts.keys():\n",
    "                word_counts[word]+=1\n",
    "            else:\n",
    "                word_counts[word]=1\n",
    "    i=0\n",
    "    for key,value in word_counts.items():\n",
    "        if value>=min_freq:\n",
    "            vocab[key]=i\n",
    "            i+=1\n",
    "    return vocab\n",
    "def add_starts_ends(data,n=1):\n",
    "    final_data=[]\n",
    "    for sentence in data:\n",
    "        final_data.append(['<s>'] * n + sentence + ['<e>'])\n",
    "    return final_data\n",
    "def suggest_word(sentence,lm):\n",
    "    max_prob=-10000\n",
    "    suggested_word=''\n",
    "    sentence_log_prob=sum([np.log(lm[(sentence[i],sentence[i+1])]) for i in range(len(sentence)-1) if (sentence[i],sentence[i+1]) in lm.keys()])\n",
    "    for key,value in lm.items():\n",
    "        if key[0]==sentence[len(sentence)-1]:\n",
    "            if np.log(value)+sentence_log_prob>max_prob:\n",
    "                max_prob=np.log(value)+sentence_log_prob\n",
    "                suggested_word=key[1]\n",
    "    return suggested_word\n",
    "\n",
    "def one_hot_encode(word,vocab):\n",
    "    word_encoded=np.zeros(len(vocab))\n",
    "    word_encoded[vocab[word]]=1\n",
    "    return(word_encoded)\n",
    "def get_input(sentence,i,vocab):\n",
    "    sentence_encoded=np.zeros(len(vocab))\n",
    "    for j in range(i-1,i):\n",
    "        sentence_encoded+=one_hot_encode(sentence[j],vocab)\n",
    "    return(sentence_encoded/(j+1))\n",
    "def prepare_train_data(data,vocab):\n",
    "    y=[]\n",
    "    x=[]\n",
    "    for sentence in data:\n",
    "        for i in range(1,len(sentence)):\n",
    "            y.append(one_hot_encode(sentence[i],vocab))\n",
    "            x.append(get_input(sentence,i,vocab))\n",
    "    return np.array(x),np.array(y)\n",
    "def get_random_batch(data,BATCH_SIZE):\n",
    "    batch=[]\n",
    "    for _ in range(BATCH_SIZE):\n",
    "        batch.append(random.choice(data))\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence\n",
      "Climate Change\n",
      "Quantum Computing\n",
      "World War II\n",
      "Ancient Egypt\n",
      "skipped: Space Exploration\n",
      "Global Health\n",
      "Economics\n",
      "Philosophy of Science\n",
      "Modern Art\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahdi/anaconda3/envs/myenv/lib/python3.11/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/mahdi/anaconda3/envs/myenv/lib/python3.11/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: Genetics\n",
      "Renewable Energy\n",
      "Cybersecurity\n",
      "Cryptocurrency\n",
      "Social Media\n",
      "Cultural Anthropology\n",
      "Astrophysics\n",
      "Human Rights\n",
      "skipped: Machine Learning\n",
      "History of Technology\n",
      "Biotechnology\n"
     ]
    }
   ],
   "source": [
    "corpus=''\n",
    "for topic in topics:\n",
    "    try:\n",
    "        page = wikipedia.page(topic)\n",
    "        corpus+=page.content\n",
    "        print(topic)\n",
    "    except:\n",
    "        print('skipped:',topic)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18384\n"
     ]
    }
   ],
   "source": [
    "data=divide_corpus(corpus)\n",
    "data=tokenize_sentences(data)\n",
    "data=add_starts_ends(data)\n",
    "vocab=get_vocabulary(data,1)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\"\n",
    "class LLM(nn.Module):\n",
    "    def __init__(self,in_features):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, in_features),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x=x.to(device)\n",
    "        return self.net(x)\n",
    "in_features=len(vocab)\n",
    "model=LLM(in_features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0\n",
      "100\n",
      "0.05217391304347826\n",
      "200\n",
      "0.06756756756756757\n",
      "300\n",
      "0.03128911138923655\n",
      "400\n",
      "0.04040404040404041\n",
      "500\n",
      "0.05473372781065089\n",
      "600\n",
      "0.05071315372424723\n",
      "700\n",
      "0.06311360448807854\n",
      "800\n",
      "0.043184885290148446\n",
      "900\n",
      "0.041720990873533245\n",
      "1000\n",
      "0.06967213114754098\n",
      "1100\n",
      "0.005008347245409015\n",
      "1200\n",
      "0.0440771349862259\n",
      "1300\n",
      "0.059654631083202514\n",
      "1400\n",
      "0.04644412191582003\n",
      "1500\n",
      "0.020348837209302327\n",
      "1600\n",
      "0.04923076923076923\n",
      "1700\n",
      "0.05860805860805861\n",
      "1800\n",
      "0.051209103840682786\n",
      "1900\n",
      "0.06002728512960437\n",
      "2000\n",
      "0.06044905008635579\n",
      "2100\n",
      "0.05443234836702955\n",
      "2200\n",
      "0.04664723032069971\n",
      "2300\n",
      "0.052795031055900624\n",
      "2400\n",
      "0.036741214057507986\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      9\u001b[0m batch\u001b[38;5;241m=\u001b[39mget_random_batch(data,BATCH_SIZE)\n\u001b[0;32m---> 10\u001b[0m x,y\u001b[38;5;241m=\u001b[39m\u001b[43mprepare_train_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m x\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfrom_numpy(x)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m y\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfrom_numpy(y)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[27], line 69\u001b[0m, in \u001b[0;36mprepare_train_data\u001b[0;34m(data, vocab)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(sentence)):\n\u001b[1;32m     68\u001b[0m         y\u001b[38;5;241m.\u001b[39mappend(one_hot_encode(sentence[i],vocab))\n\u001b[0;32m---> 69\u001b[0m         x\u001b[38;5;241m.\u001b[39mappend(\u001b[43mget_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(x),np\u001b[38;5;241m.\u001b[39marray(y)\n",
      "Cell \u001b[0;32mIn[27], line 61\u001b[0m, in \u001b[0;36mget_input\u001b[0;34m(sentence, i, vocab)\u001b[0m\n\u001b[1;32m     59\u001b[0m sentence_encoded\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(vocab))\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,i):\n\u001b[0;32m---> 61\u001b[0m     sentence_encoded\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mone_hot_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(sentence_encoded\u001b[38;5;241m/\u001b[39m(j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[27], line 55\u001b[0m, in \u001b[0;36mone_hot_encode\u001b[0;34m(word, vocab)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_hot_encode\u001b[39m(word,vocab):\n\u001b[0;32m---> 55\u001b[0m     word_encoded\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(vocab))\n\u001b[1;32m     56\u001b[0m     word_encoded[vocab[word]]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(word_encoded)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS=50000\n",
    "BATCH_SIZE=32\n",
    "best_accuracy=0\n",
    "crit=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-2,weight_decay=5e-4)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    correct = 0\n",
    "    batch=get_random_batch(data,BATCH_SIZE)\n",
    "    x,y=prepare_train_data(batch,vocab)\n",
    "    x=torch.from_numpy(x).float().to(device)\n",
    "    y=torch.from_numpy(y).float().to(device)\n",
    "    #forward\n",
    "    y_hat=model(x).to(device)\n",
    "    loss=crit(y,y_hat)\n",
    "    #backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, predicted = torch.max(y_hat, 1)\n",
    "    y_indices = torch.argmax(y, dim=1)\n",
    "    correct = (predicted == y_indices).sum().item()\n",
    "    if epoch%100==0:\n",
    "        print(epoch)\n",
    "        print(correct/len(x))\n",
    "    \n",
    "    # accuracy = correct / total\n",
    "    # if accuracy>=best_accuracy:\n",
    "    #     best_accuracy=accuracy\n",
    "    # print(epoch)\n",
    "    # print('accuracy: ',accuracy)\n",
    "    # print('best_accuracy: ',best_accuracy)\n",
    "    del x\n",
    "    del y\n",
    "    del batch\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence='how are'\n",
    "my_sentence=divide_corpus(my_sentence)\n",
    "my_sentence=tokenize_sentences(my_sentence)\n",
    "my_sentence=add_starts_ends(my_sentence)\n",
    "\n",
    "x,y=prepare_train_data(my_sentence,vocab)\n",
    "x=x[len(x)-1]\n",
    "x=torch.from_numpy(x).float().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "y_hat=model(x).to(device)\n",
    "# _, predicted = torch.max(y_hat, 1)\n",
    "# y_indices = torch.argmax(y, dim=1)\n",
    "# y_indices\n",
    "max_index = y_hat.argmax()\n",
    "\n",
    "print(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<e>\n"
     ]
    }
   ],
   "source": [
    "for key,value in vocab.items():\n",
    "    if value==15:\n",
    "        print(key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
